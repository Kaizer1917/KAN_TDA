# KAN_TDA: KAN-based Frequency Decomposition Learning Architecture

<div align="center">
  <h2><b> (ICLR 2025) KAN_TDA: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting ğŸš€ </b></h2>
</div>

**Official implementation of "KAN_TDA: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting"**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org/)

## ğŸ“– Overview

KAN_TDA introduces a novel **KAN-based Frequency Decomposition Learning architecture** that revolutionizes long-term time series forecasting by addressing the challenge of mixed frequency components in real-world time series data. The model achieves **state-of-the-art performance** while being **extremely lightweight** (12.84K-38.12K parameters vs competitors with 75K-25M parameters).

### ğŸ¯ Key Innovations

1. **Frequency Decomposition Approach**: Decomposes mixed frequency components into multiple single frequency components for independent modeling
2. **Multi-order KAN Integration**: First application of Kolmogorov-Arnold Networks (KAN) to time series forecasting with adaptive complexity matching
3. **Decomposition-Learning-Mixing Framework**: Cascaded process that decomposes â†’ learns â†’ mixes frequency components

## ğŸ—ï¸ Architecture Overview


The KAN_TDA architecture consists of three main components:

### 1. **Hierarchical Sequence Preprocessing**
```
x_i = AvgPool(Padding(x_{i-1}))   for i âˆˆ {1,...,k}
x_i = Linear(x_i) â†’ x_i âˆˆ R^{T/d^{i-1} Ã— D}
```
- Progressive downsampling using moving averages
- Multi-level sequences: `{x_1, x_2, ..., x_k}` (high â†’ low frequency)
- Each sequence embedded into higher dimension `D`

### 2. **Cascaded Frequency Decomposition (CFD) Blocks**
```
xÌ‚_i = IFFT(Padding(FFT(x_{i+1})))     # Frequency Upsampling
f_i = x_i - xÌ‚_i                        # i-th frequency component extraction
```
**Key Innovation - Lossless Frequency Upsampling:**
- Uses FFT â†’ Zero-Padding â†’ IFFT to preserve frequency information
- Residual extraction isolates specific frequency bands
- No information degradation during upsampling

### 3. **Multi-order KAN Representation Learning (M-KAN) Blocks**

**Dual-Branch Architecture:**

**Branch 1 - Temporal Dependencies:**
```
f_{i,1} = Conv_{Dâ†’D}(f_i, group=D)  # Depthwise Convolution
```

**Branch 2 - KAN Representation Learning:**
```
T_n(x) = cos(n Ã— arccos(x))                    # Chebyshev polynomial
Ï†_o(x) = Î£_{j=1}^D Î£_{i=0}^n Î˜_{o,j,i} T_i(tanh(x_j))   # Learnable function
f_{i,2} = KAN(f_i, order=b+k-i)               # Multi-order adaptation
```

**Multi-order Strategy:**
- **Low frequencies â†’ Low-order KANs**: Simple patterns, minimal complexity
- **High frequencies â†’ High-order KANs**: Complex patterns, higher representation capacity
- **Adaptive complexity**: `Order(frequency_level_i) = base_order + (total_levels - i)`

**Final Output:**
```
fÌ‚_i = f_{i,1} + f_{i,2}  # Residual connection
```

### 4. **Frequency Mixing Blocks**
```
x_i = IFFT(Padding(FFT(x_{i+1}))) + f_i    # Reconstruct multi-level sequences
X_O = Linear(x_1)                          # Final prediction
```

## ğŸ§® Mathematical Theory

### Kolmogorov-Arnold Network (KAN) Foundation
- **Kolmogorov-Arnold Representation Theorem**: Any multivariate continuous function can be expressed as combinations of univariate functions and additions
- **KAN vs MLP**: Replaces fixed node activations with learnable edge functions
- **Implementation**: `z_{l+1,j} = Î£_{i=1}^{n_l} Ï†_{l,j,i}(z_{l,i})`

### Frequency Decomposition Mathematics
**Core Principle:**
```
FFT: Time Domain â†’ Frequency Domain
Padding: Extend frequency spectrum  
IFFT: Frequency Domain â†’ Time Domain (preserved characteristics)
```

**Frequency Band Isolation:**
- Level `i` sequence: `x_i` (contains frequencies 1 to i)
- Level `i+1` sequence: `x_{i+1}` (contains frequencies 1 to i-1)
- Frequency upsampling: `xÌ‚_i` (reconstructed without i-th frequency)
- Isolated i-th frequency: `f_i = x_i - xÌ‚_i`

## ğŸ“Š Performance Results


### Key Achievements:
- **State-of-the-art performance** across multiple datasets (ETTh1, ETTh2, ETTm1, ETTm2, Weather)
- **Extremely lightweight**: 12.84K-38.12K parameters vs competitors with 75K-25M parameters
- **Computational efficiency**: 7.63M-29.86M MACs vs competitors with 20M-35G MACs
- **Consistent improvements** with longer look-back windows

### Ablation Study Findings:
1. **Frequency Upsampling is irreplaceable**: Alternatives degrade performance significantly
2. **Multi-order KANs outperform**: Both fixed-order KANs and MLPs
3. **Depthwise Convolution optimal**: Better than standard convolution or self-attention

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install -r requirements.txt
```

### Dataset Preparation
Download all datasets from [Autoformer](https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy).

### Training
All training scripts are in the `./scripts` directory. 

**Example**: To train on Weather dataset with input-96-predict-96:
```bash
sh scripts/long_term_forecast/Weather/weather_96.sh
```

### Model Configuration
Key parameters in `run.py`:
```python
# Model Architecture
d_model: int = 16              # Embedding dimension
e_layers: int = 2              # Number of decomposition-mixing blocks
down_sampling_layers: int = 0  # Number of frequency levels
begin_order: int = 1           # Base KAN polynomial order

# Time Series
seq_len: int = 96              # Input sequence length
pred_len: int = 96             # Prediction length
moving_avg: int = 25           # Moving average window
```

## ğŸ“ Project Structure

```
KAN_TDA/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ KAN_TDA.py              # Main model architecture
â”œâ”€â”€ layers/
â”‚   â”œâ”€â”€ ChebyKANLayer.py        # Chebyshev KAN implementation
â”‚   â”œâ”€â”€ Embed.py                # Embedding layers
â”‚   â”œâ”€â”€ StandardNorm.py         # Normalization utilities
â”‚   â”œâ”€â”€ TakensEmbedding.py      # TDA: Takens embedding
â”‚   â””â”€â”€ PersistentHomology.py   # TDA: Persistent homology
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ persistence_landscapes.py  # TDA: Persistence landscapes
â”œâ”€â”€ exp/
â”‚   â”œâ”€â”€ exp_basic.py            # Base experiment class
â”‚   â””â”€â”€ exp_long_term_forecasting.py  # Training/evaluation logic
â”œâ”€â”€ data_provider/              # Data loading and preprocessing
â”œâ”€â”€ scripts/                    # Training scripts
â””â”€â”€ run.py                      # Main entry point
```

## ğŸ”¬ TDA Integration (Advanced Features)

This implementation includes **Topological Data Analysis (TDA)** integration for enhanced pattern recognition:

- **Takens Embedding**: Transform time series into point clouds preserving topological properties
- **Persistent Homology**: Capture birth-death of topological features (components, loops, voids)
- **Persistence Landscapes**: Convert topological features to ML-ready statistical summaries

### TDA Components:
- `TakensEmbedding`: Multi-scale delay embedding with automatic parameter optimization
- `PersistentHomology`: Multi-backend homology computation (Ripser, GUDHI, Giotto)
- `PersistenceLandscape`: Statistical feature extraction from persistence diagrams

## ğŸ”§ Extension Points

The architecture is designed for easy extension:

1. **New KAN Variants**: Replace Chebyshev with other basis functions (Fourier, Legendre)
2. **Alternative Decomposition**: Wavelet decomposition, empirical mode decomposition
3. **Attention Mechanisms**: Cross-frequency attention, temporal attention
4. **TDA Integration**: Topology-guided frequency decomposition and KAN adaptation

